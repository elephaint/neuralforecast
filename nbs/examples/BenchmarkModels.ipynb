{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Models\n",
    "> Tutorial on how to benchmark neuralforecast models on multiple datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking is crucial for time series forecasting: we want to evaluate models across different datasets, with different settings, to better understand model behaviour and help us pick the right model for a task. \n",
    "\n",
    "In this notebook, we show how to benchmark a set of neuralforecast models on a set of commonly used benchmark time series datasets from the academic literature. \n",
    "\n",
    "We will show how to:\n",
    "* Load a set of benchmark datasets, used in the academic literature.\n",
    "* Train a set of models on these datasets.\n",
    "* Forecast the test set.\n",
    "* Evaluate performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run these experiments using GPU with Google Colab.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/Nixtla/neuralforecast/blob/main/nbs/examples/LongHorizon_with_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install neuralforecast datasetsforecast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data, Models, Losses and Metrics\n",
    "\n",
    "The `LongHorizon` class will automatically download a set of benchmark datasets and process it. In this example, we will benchmark `NHITS`, `BiTCN`, `TSMixer`, `DLinear` and `iTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasetsforecast.long_horizon import LongHorizon, LongHorizonInfo\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import NHITS, BiTCN, TSMixer, DLinear, iTransformer\n",
    "from neuralforecast.losses.pytorch import MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models\n",
    "\n",
    "We create a `load_models` function that will return a list of models to evaluate given an output forecast horizon, input size and seed. Feel free to add your model to the list in the function; make sure to import the model in the above import statements. We will use the models mostly with their default settings in this example; only the `scaler_type` is different for some of the models.\n",
    "\n",
    "Note that `TSMixer` and `iTransformer` are multivariate models, which means they require an additional `n_series` parameter as these models will forecast all series in the dataset concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def load_models(horizon, input_size, n_series, seed):\n",
    "    models = [              \n",
    "               NHITS(h=horizon,\n",
    "                    input_size=input_size,\n",
    "                    early_stop_patience_steps=5,\n",
    "                    scaler_type='robust',\n",
    "                    valid_loss=MAE(),\n",
    "                    random_seed=seed,\n",
    "                    ),   \n",
    "               DLinear(h=horizon,\n",
    "                    input_size=input_size,\n",
    "                    max_steps=1000,\n",
    "                    early_stop_patience_steps=5,\n",
    "                    scaler_type='standard',\n",
    "                    valid_loss=MAE(),\n",
    "                    random_seed=seed,\n",
    "                    ), \n",
    "               BiTCN(h=horizon,\n",
    "                    input_size=input_size,\n",
    "                    early_stop_patience_steps=5,\n",
    "                    scaler_type='standard',\n",
    "                    valid_loss=MAE(),\n",
    "                    random_seed=seed,\n",
    "                    ),         \n",
    "               TSMixer(h=horizon,\n",
    "                    input_size=input_size,\n",
    "                    n_series=n_series,\n",
    "                    early_stop_patience_steps=5,\n",
    "                    scaler_type='identity',\n",
    "                    valid_loss=MAE(),\n",
    "                    random_seed=seed,\n",
    "                    ),                                                                                           \n",
    "               iTransformer(h=horizon,\n",
    "                    input_size=input_size,\n",
    "                    n_series=n_series,\n",
    "                    early_stop_patience_steps=5,\n",
    "                    scaler_type='identity',\n",
    "                    valid_loss=MAE(),\n",
    "                    random_seed=seed,\n",
    "                    ),                                  \n",
    "          ]\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train models\n",
    "\n",
    "We will train the models in a cross-validation procedure for a given dataset, horizon, input size and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def cross_validation(results, dataset, horizon, input_size, metrics, seed=1234567):\n",
    "    \n",
    "    # Access the frequency, validation size, test_size and n_series of the dataset\n",
    "    freq = LongHorizonInfo[dataset].freq\n",
    "    val_size = LongHorizonInfo[dataset].val_size\n",
    "    test_size = LongHorizonInfo[dataset].test_size\n",
    "    n_series = LongHorizonInfo[dataset].n_ts  \n",
    "\n",
    "    # Load the dataset\n",
    "    Y_df, _, _ = LongHorizon.load(directory='./', group=dataset)\n",
    "    Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "\n",
    "    # Create the model list\n",
    "    models = load_models(horizon, input_size, n_series, seed=seed)\n",
    "\n",
    "    # Instantiate NeuralForecast\n",
    "    nf = NeuralForecast(\n",
    "        models=models,\n",
    "        freq=freq)   \n",
    "\n",
    "    # Create a set of forecasts using cross-validation\n",
    "    Y_hat_df = nf.cross_validation(df=Y_df,\n",
    "                                val_size=val_size,\n",
    "                                test_size=test_size,\n",
    "                                n_windows=None)                                 \n",
    "    Y_hat_df = Y_hat_df.reset_index()    \n",
    "\n",
    "    # Save the metric results to a dictionary\n",
    "    for model in models:\n",
    "        results[dataset][horizon][model] = {}\n",
    "        for metric, fmetric in metrics.items():\n",
    "            metric_model = fmetric(Y_hat_df['y'], Y_hat_df[f'{model}'])\n",
    "            results[dataset][horizon][model][metric] = metric_model\n",
    "\n",
    "    return results\n",
    "\n",
    "# Helper function to process the dictionary of results in the end\n",
    "# https://stackoverflow.com/questions/47416113/how-to-build-a-multiindex-pandas-dataframe-from-a-nested-dictionary-with-lists    \n",
    "def get_result_df(results):\n",
    "    d = results\n",
    "    d = {(i, j, f'{k}'): d[i][j][k] \n",
    "        for i in d.keys() \n",
    "        for j in d[i].keys()\n",
    "        for k in d[i][j].keys()}     \n",
    "\n",
    "    mux = pd.MultiIndex.from_tuples(d.keys())\n",
    "    df = pd.DataFrame(list(d.values()), index=mux).stack().reset_index()\n",
    "    df.columns = ['dataset', 'horizon', 'model', 'metric', 'value']\n",
    "    df['value'] = df['value'].round(3)\n",
    "    df['dataset'] = pd.Categorical(df['dataset'])\n",
    "    df['horizon'] = pd.Categorical(df['horizon'])\n",
    "    df['model'] = pd.Categorical(df['model'])\n",
    "    df['metric'] = pd.Categorical(df['metric'])\n",
    "    df = df.set_index(['dataset', 'horizon', 'metric', 'model'])\n",
    "    df = df.unstack('metric').unstack('model')    \n",
    "\n",
    "    return df        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Running the benchmark\n",
    "\n",
    "First, we define all our experimental settings:\n",
    "- A set of datasets from `LongHorizon`\n",
    "- The input size (sequence length) to the models\n",
    "- A set of metrics to evaluate\n",
    "\n",
    "In this example, we will only evaluate on the `ETTm1` dataset. You can uncomment the other datasets to include them in the benchmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "Note that benchmarking may take a long time and require a high amount of resources.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.losses.numpy import mse, mae, smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Define dictionary of datasets to evaluate. The following dictionary includes all available datasets; uncomment those you wish to include in the benchmark.\n",
    "datasets = {\n",
    "            # 'ETTh1',\n",
    "            # 'ETTh2',\n",
    "            'ETTm1',\n",
    "            # 'ETTm2',\n",
    "            # 'ECL',\n",
    "            # 'TrafficL',\n",
    "            # 'Weather',\n",
    "            # 'ILI',\n",
    "            }\n",
    "\n",
    "# Input_size and metrics to evaluate.\n",
    "input_size = 96\n",
    "metrics = {'MSE': mse, \n",
    "           'MAE': mae, \n",
    "           'sMAPE': smape}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the benchmark experiment. \n",
    "\n",
    "The following code will loop over all datasets, and over all horizons that each dataset is commonly evaluated on (this is provided as an attribute in `LongHorizonInfo`). It will then cross-validate the set of models for each dataset-horizon combination, and return the metrics on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name         | Type          | Params\n",
      "-----------------------------------------------\n",
      "0 | loss         | MAE           | 0     \n",
      "1 | valid_loss   | MAE           | 0     \n",
      "2 | padder_train | ConstantPad1d | 0     \n",
      "3 | scaler       | TemporalNorm  | 0     \n",
      "4 | blocks       | ModuleList    | 2.7 M \n",
      "-----------------------------------------------\n",
      "2.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.7 M     Total params\n",
      "10.791    Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | loss          | MAE           | 0     \n",
      "1 | valid_loss    | MAE           | 0     \n",
      "2 | padder_train  | ConstantPad1d | 0     \n",
      "3 | scaler        | TemporalNorm  | 0     \n",
      "4 | decomp        | SeriesDecomp  | 0     \n",
      "5 | linear_trend  | Linear        | 9.3 K \n",
      "6 | linear_season | Linear        | 9.3 K \n",
      "------------------------------------------------\n",
      "18.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "18.6 K    Total params\n",
      "0.074     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "   | Name          | Type          | Params\n",
      "-------------------------------------------------\n",
      "0  | loss          | MAE           | 0     \n",
      "1  | valid_loss    | MAE           | 0     \n",
      "2  | padder_train  | ConstantPad1d | 0     \n",
      "3  | scaler        | TemporalNorm  | 0     \n",
      "4  | lin_hist      | Linear        | 32    \n",
      "5  | drop_hist     | Dropout       | 0     \n",
      "6  | net_bwd       | Sequential    | 7.5 K \n",
      "7  | drop_temporal | Dropout       | 0     \n",
      "8  | temporal_lin1 | Linear        | 1.6 K \n",
      "9  | temporal_lin2 | Linear        | 1.6 K \n",
      "10 | output_lin    | Linear        | 17    \n",
      "-------------------------------------------------\n",
      "10.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "10.7 K    Total params\n",
      "0.043     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type                     | Params\n",
      "-----------------------------------------------------------\n",
      "0 | loss          | MAE                      | 0     \n",
      "1 | valid_loss    | MAE                      | 0     \n",
      "2 | padder        | ConstantPad1d            | 0     \n",
      "3 | scaler        | TemporalNorm             | 0     \n",
      "4 | norm          | ReversibleInstanceNorm1d | 14    \n",
      "5 | mixing_layers | Sequential               | 25.9 K\n",
      "6 | out           | Linear                   | 9.3 K \n",
      "-----------------------------------------------------------\n",
      "35.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "35.3 K    Total params\n",
      "0.141     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type                   | Params\n",
      "---------------------------------------------------------\n",
      "0 | loss          | MAE                    | 0     \n",
      "1 | valid_loss    | MAE                    | 0     \n",
      "2 | padder        | ConstantPad1d          | 0     \n",
      "3 | scaler        | TemporalNorm           | 0     \n",
      "4 | enc_embedding | DataEmbedding_inverted | 49.7 K\n",
      "5 | encoder       | TransEncoder           | 6.3 M \n",
      "6 | projector     | Linear                 | 49.2 K\n",
      "---------------------------------------------------------\n",
      "6.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.4 M     Total params\n",
      "25.619    Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name         | Type          | Params\n",
      "-----------------------------------------------\n",
      "0 | loss         | MAE           | 0     \n",
      "1 | valid_loss   | MAE           | 0     \n",
      "2 | padder_train | ConstantPad1d | 0     \n",
      "3 | scaler       | TemporalNorm  | 0     \n",
      "4 | blocks       | ModuleList    | 2.8 M \n",
      "-----------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.135    Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | loss          | MAE           | 0     \n",
      "1 | valid_loss    | MAE           | 0     \n",
      "2 | padder_train  | ConstantPad1d | 0     \n",
      "3 | scaler        | TemporalNorm  | 0     \n",
      "4 | decomp        | SeriesDecomp  | 0     \n",
      "5 | linear_trend  | Linear        | 18.6 K\n",
      "6 | linear_season | Linear        | 18.6 K\n",
      "------------------------------------------------\n",
      "37.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "37.2 K    Total params\n",
      "0.149     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "   | Name          | Type          | Params\n",
      "-------------------------------------------------\n",
      "0  | loss          | MAE           | 0     \n",
      "1  | valid_loss    | MAE           | 0     \n",
      "2  | padder_train  | ConstantPad1d | 0     \n",
      "3  | scaler        | TemporalNorm  | 0     \n",
      "4  | lin_hist      | Linear        | 32    \n",
      "5  | drop_hist     | Dropout       | 0     \n",
      "6  | net_bwd       | Sequential    | 7.5 K \n",
      "7  | drop_temporal | Dropout       | 0     \n",
      "8  | temporal_lin1 | Linear        | 1.6 K \n",
      "9  | temporal_lin2 | Linear        | 3.3 K \n",
      "10 | output_lin    | Linear        | 17    \n",
      "-------------------------------------------------\n",
      "12.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.4 K    Total params\n",
      "0.049     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type                     | Params\n",
      "-----------------------------------------------------------\n",
      "0 | loss          | MAE                      | 0     \n",
      "1 | valid_loss    | MAE                      | 0     \n",
      "2 | padder        | ConstantPad1d            | 0     \n",
      "3 | scaler        | TemporalNorm             | 0     \n",
      "4 | norm          | ReversibleInstanceNorm1d | 14    \n",
      "5 | mixing_layers | Sequential               | 25.9 K\n",
      "6 | out           | Linear                   | 18.6 K\n",
      "-----------------------------------------------------------\n",
      "44.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "44.6 K    Total params\n",
      "0.178     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type                   | Params\n",
      "---------------------------------------------------------\n",
      "0 | loss          | MAE                    | 0     \n",
      "1 | valid_loss    | MAE                    | 0     \n",
      "2 | padder        | ConstantPad1d          | 0     \n",
      "3 | scaler        | TemporalNorm           | 0     \n",
      "4 | enc_embedding | DataEmbedding_inverted | 49.7 K\n",
      "5 | encoder       | TransEncoder           | 6.3 M \n",
      "6 | projector     | Linear                 | 98.5 K\n",
      "---------------------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "25.816    Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name         | Type          | Params\n",
      "-----------------------------------------------\n",
      "0 | loss         | MAE           | 0     \n",
      "1 | valid_loss   | MAE           | 0     \n",
      "2 | padder_train | ConstantPad1d | 0     \n",
      "3 | scaler       | TemporalNorm  | 0     \n",
      "4 | blocks       | ModuleList    | 2.9 M \n",
      "-----------------------------------------------\n",
      "2.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.9 M     Total params\n",
      "11.653    Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | loss          | MAE           | 0     \n",
      "1 | valid_loss    | MAE           | 0     \n",
      "2 | padder_train  | ConstantPad1d | 0     \n",
      "3 | scaler        | TemporalNorm  | 0     \n",
      "4 | decomp        | SeriesDecomp  | 0     \n",
      "5 | linear_trend  | Linear        | 32.6 K\n",
      "6 | linear_season | Linear        | 32.6 K\n",
      "------------------------------------------------\n",
      "65.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "65.2 K    Total params\n",
      "0.261     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "   | Name          | Type          | Params\n",
      "-------------------------------------------------\n",
      "0  | loss          | MAE           | 0     \n",
      "1  | valid_loss    | MAE           | 0     \n",
      "2  | padder_train  | ConstantPad1d | 0     \n",
      "3  | scaler        | TemporalNorm  | 0     \n",
      "4  | lin_hist      | Linear        | 32    \n",
      "5  | drop_hist     | Dropout       | 0     \n",
      "6  | net_bwd       | Sequential    | 7.5 K \n",
      "7  | drop_temporal | Dropout       | 0     \n",
      "8  | temporal_lin1 | Linear        | 1.6 K \n",
      "9  | temporal_lin2 | Linear        | 5.7 K \n",
      "10 | output_lin    | Linear        | 17    \n",
      "-------------------------------------------------\n",
      "14.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "14.8 K    Total params\n",
      "0.059     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type                     | Params\n",
      "-----------------------------------------------------------\n",
      "0 | loss          | MAE                      | 0     \n",
      "1 | valid_loss    | MAE                      | 0     \n",
      "2 | padder        | ConstantPad1d            | 0     \n",
      "3 | scaler        | TemporalNorm             | 0     \n",
      "4 | norm          | ReversibleInstanceNorm1d | 14    \n",
      "5 | mixing_layers | Sequential               | 25.9 K\n",
      "6 | out           | Linear                   | 32.6 K\n",
      "-----------------------------------------------------------\n",
      "58.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "58.5 K    Total params\n",
      "0.234     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type                   | Params\n",
      "---------------------------------------------------------\n",
      "0 | loss          | MAE                    | 0     \n",
      "1 | valid_loss    | MAE                    | 0     \n",
      "2 | padder        | ConstantPad1d          | 0     \n",
      "3 | scaler        | TemporalNorm           | 0     \n",
      "4 | enc_embedding | DataEmbedding_inverted | 49.7 K\n",
      "5 | encoder       | TransEncoder           | 6.3 M \n",
      "6 | projector     | Linear                 | 172 K \n",
      "---------------------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n",
      "26.111    Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:lightning_fabric.utilities.seed:Seed set to 1234567\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name         | Type          | Params\n",
      "-----------------------------------------------\n",
      "0 | loss         | MAE           | 0     \n",
      "1 | valid_loss   | MAE           | 0     \n",
      "2 | padder_train | ConstantPad1d | 0     \n",
      "3 | scaler       | TemporalNorm  | 0     \n",
      "4 | blocks       | ModuleList    | 3.3 M \n",
      "-----------------------------------------------\n",
      "3.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.3 M     Total params\n",
      "13.031    Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | loss          | MAE           | 0     \n",
      "1 | valid_loss    | MAE           | 0     \n",
      "2 | padder_train  | ConstantPad1d | 0     \n",
      "3 | scaler        | TemporalNorm  | 0     \n",
      "4 | decomp        | SeriesDecomp  | 0     \n",
      "5 | linear_trend  | Linear        | 69.8 K\n",
      "6 | linear_season | Linear        | 69.8 K\n",
      "------------------------------------------------\n",
      "139 K     Trainable params\n",
      "0         Non-trainable params\n",
      "139 K     Total params\n",
      "0.559     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "   | Name          | Type          | Params\n",
      "-------------------------------------------------\n",
      "0  | loss          | MAE           | 0     \n",
      "1  | valid_loss    | MAE           | 0     \n",
      "2  | padder_train  | ConstantPad1d | 0     \n",
      "3  | scaler        | TemporalNorm  | 0     \n",
      "4  | lin_hist      | Linear        | 32    \n",
      "5  | drop_hist     | Dropout       | 0     \n",
      "6  | net_bwd       | Sequential    | 7.5 K \n",
      "7  | drop_temporal | Dropout       | 0     \n",
      "8  | temporal_lin1 | Linear        | 1.6 K \n",
      "9  | temporal_lin2 | Linear        | 12.2 K\n",
      "10 | output_lin    | Linear        | 17    \n",
      "-------------------------------------------------\n",
      "21.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "21.3 K    Total params\n",
      "0.085     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type                     | Params\n",
      "-----------------------------------------------------------\n",
      "0 | loss          | MAE                      | 0     \n",
      "1 | valid_loss    | MAE                      | 0     \n",
      "2 | padder        | ConstantPad1d            | 0     \n",
      "3 | scaler        | TemporalNorm             | 0     \n",
      "4 | norm          | ReversibleInstanceNorm1d | 14    \n",
      "5 | mixing_layers | Sequential               | 25.9 K\n",
      "6 | out           | Linear                   | 69.8 K\n",
      "-----------------------------------------------------------\n",
      "95.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "95.8 K    Total params\n",
      "0.383     Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name          | Type                   | Params\n",
      "---------------------------------------------------------\n",
      "0 | loss          | MAE                    | 0     \n",
      "1 | valid_loss    | MAE                    | 0     \n",
      "2 | padder        | ConstantPad1d          | 0     \n",
      "3 | scaler        | TemporalNorm           | 0     \n",
      "4 | enc_embedding | DataEmbedding_inverted | 49.7 K\n",
      "5 | encoder       | TransEncoder           | 6.3 M \n",
      "6 | projector     | Linear                 | 369 K \n",
      "---------------------------------------------------------\n",
      "6.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 M     Total params\n",
      "26.899    Total estimated model params size (MB)\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1000` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "results = {}\n",
    "for dataset in datasets:\n",
    "    results[dataset] = {}\n",
    "    horizons = LongHorizonInfo[dataset].horizons\n",
    "    for horizon in horizons:\n",
    "        results[dataset][horizon] = {}\n",
    "        results = cross_validation(results, dataset, horizon, input_size, metrics, seed=1234567)\n",
    "\n",
    "df_results = get_result_df(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "The results are returned in a pandas Dataframe `df_results`. You can compare these results to the results reported in the respective papers of these methods.\n",
    "\n",
    "As you can see, it's a tight battle between these methods on `ETTm1`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"15\" halign=\"left\">value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th colspan=\"5\" halign=\"left\">MAE</th>\n",
       "      <th colspan=\"5\" halign=\"left\">MSE</th>\n",
       "      <th colspan=\"5\" halign=\"left\">sMAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>BiTCN</th>\n",
       "      <th>DLinear</th>\n",
       "      <th>NHITS</th>\n",
       "      <th>TSMixer</th>\n",
       "      <th>iTransformer</th>\n",
       "      <th>BiTCN</th>\n",
       "      <th>DLinear</th>\n",
       "      <th>NHITS</th>\n",
       "      <th>TSMixer</th>\n",
       "      <th>iTransformer</th>\n",
       "      <th>BiTCN</th>\n",
       "      <th>DLinear</th>\n",
       "      <th>NHITS</th>\n",
       "      <th>TSMixer</th>\n",
       "      <th>iTransformer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>horizon</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ETTm1</th>\n",
       "      <th>96</th>\n",
       "      <td>0.361</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.376</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.702</td>\n",
       "      <td>0.671</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.383</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.701</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.402</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.737</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.436</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 value                                                     \\\n",
       "metric             MAE                                        MSE           \n",
       "model            BiTCN DLinear  NHITS TSMixer iTransformer  BiTCN DLinear   \n",
       "dataset horizon                                                             \n",
       "ETTm1   96       0.361   0.365  0.350   0.351        0.376  0.340   0.349   \n",
       "        192      0.383   0.386  0.379   0.374        0.398  0.383   0.391   \n",
       "        336      0.402   0.406  0.409   0.395        0.415  0.414   0.423   \n",
       "        720      0.436   0.444  0.446   0.431        0.450  0.473   0.490   \n",
       "\n",
       "                                                                           \\\n",
       "metric                                       sMAPE                          \n",
       "model            NHITS TSMixer iTransformer  BiTCN DLinear  NHITS TSMixer   \n",
       "dataset horizon                                                             \n",
       "ETTm1   96       0.323   0.334        0.352  0.694   0.702  0.671   0.681   \n",
       "        192      0.377   0.381        0.397  0.716   0.723  0.701   0.703   \n",
       "        336      0.423   0.412        0.425  0.737   0.749  0.735   0.730   \n",
       "        720      0.481   0.476        0.485  0.775   0.794  0.787   0.776   \n",
       "\n",
       "                              \n",
       "metric                        \n",
       "model           iTransformer  \n",
       "dataset horizon               \n",
       "ETTm1   96             0.715  \n",
       "        192            0.749  \n",
       "        336            0.757  \n",
       "        720            0.800  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
